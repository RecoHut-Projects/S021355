{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RecoHut-Stanzas/S021355/blob/main/reports/S021355.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "source": [
        "# Matching and Ranking models in Tensorflow\n",
        "\n",
        "***Matching models on ML-1m and ranking models on Criteo (sample) dataset in TF v2.5***\n",
        "\n",
        "## Matching models on Movielens ML-1m dataset\n",
        "\n",
        "### 1. Bayesian Personalized Ranking (BPR)\n",
        "\n",
        "In matrix factorization (MF), to compute the prediction we have to multiply the user factors to the item factors:\n",
        "\n",
        "$$\\hat{x}_{ui} = \\langle w_u,h_i \\rangle = \\sum_{f=1}^k w_{uf} \\cdot h_{if}$$\n",
        "\n",
        "The usual approach for item recommenders is to predict a personalized score $\\hat{x}_{ui}$ for an item that reflects the preference of the user for the item. Then the items are ranked by sorting them according to that score. Machine learning approaches are typically fit by using observed items as a positive sample and missing ones for the negative class. A perfect model would thus be useless, as it would classify as negative (non-interesting) all the items that were non-observed at training time. The only reason why such methods work is regularization.\n",
        "\n",
        "BPR use a different approach. The training dataset is composed by triplets $(u,i,j)$ representing that user $u$ is assumed to prefer $i$ over $j$. For an implicit dataset this means that $u$ observed $i$ but not $j$:\n",
        "\n",
        "$$D_S := \\{(u,i,j) \\mid i \\in I_u^+ \\wedge j \\in I \\setminus I_u^+\\}$$\n",
        "\n",
        "A machine learning model can be represented by a parameter vector $Θ$ which is found at fitting time. BPR wants to find the parameter vector that is most probable given the desired, but latent, preference structure $>_u$:\n",
        "\n",
        "$$\\begin{align} p(\\Theta \\mid >_u) \\propto p(>_u \\mid \\Theta)p(\\Theta) \\\\ \\prod_{u\\in U} p(>_u \\mid \\Theta) = \\dots = \\prod_{(u,i,j) \\in D_S} p(i >_u j \\mid \\Theta) \\end{align}$$\n",
        "\n",
        "The probability that a user really prefers item $i$ to item $j$ is defined as:\n",
        "\n",
        "$$\\begin{align} p(i >_u j \\mid \\Theta) := \\sigma(\\hat{x}_{uij}(\\Theta)) \\end{align}$$\n",
        "\n",
        "Where $σ$ represent the logistic sigmoid and $\\hat{x}_{uij}(Θ)$ is an arbitrary real-valued function of $Θ$ (the output of your arbitrary model).\n",
        "\n",
        "User $u_1$ has interacted with item $i_2$ but not item $i_1$, so we assume that this user prefers item $i_2$ over $i_1$. For items that the user have both interacted with, we cannot infer any preference. The same is true for two items that a user has not interacted yet (e.g. item $i_1$ and $i_4$ for user $u_1$).\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img0.png](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img0.png)\n",
        "\n",
        "To complete the Bayesian setting, we define a prior density for the parameters:\n",
        "\n",
        "$$p(\\Theta) \\sim N(0, \\Sigma_\\Theta)$$\n",
        "\n",
        "And we can now formulate the maximum posterior estimator:\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{split}   BPR-OPT &=\\log p(\\Theta \\mid >_u)\\\\\n",
        "      &=\\log p(>_u \\mid \\Theta) p(\\Theta) \\\\ &= \\log \\prod_{(u,i,j) \\in D_S} \\sigma(\\hat{x}_{uij})p(\\Theta) \\\\ &= \\sum_{(u,i,j) \\in D_S} \\log \\sigma(\\hat{x}_{uij}) + \\log p(\\Theta) \\\\ &= \\sum_{(u,i,j) \\in D_S} \\log \\sigma(\\hat{x}_{uij}) - \\lambda_\\Theta ||\\Theta||^2\n",
        "\\end{split}\n",
        "\\end{equation} $$\n",
        "\n",
        "Where $λ_Θ$ are model specific regularization parameters.\n",
        "\n",
        "Once obtained the log-likelihood, we need to maximize it in order to find our optimal $Θ$. As the criterion is differentiable, gradient descent algorithms are an obvious choiche for maximization.\n",
        "\n",
        "The basic version of gradient descent consists in evaluating the gradient using all the available samples and then perform a single update. The problem with this is, in our case, that our training dataset is very skewed. Suppose an item $i$ is very popular. Then we have many terms of the form $\\hat{x}_{uij}$ in the loss because for many users $u$ the item $i$ is compared against all negative items $j$. The other popular approach is stochastic gradient descent, where for each training sample an update is performed. This is a better approach, but the order in which the samples are traversed is crucial. To solve this issue BPR uses a stochastic gradient descent algorithm that chooses the triples randomly.\n",
        "\n",
        "The gradient of BPR-OPT with respect to the model parameters is:\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{split}   \\frac{\\partial BPR-OPT}{\\partial \\Theta} &= \\sum_{(u,i,j) \\in D_S} \\frac{\\partial}{\\partial \\Theta} \\log \\sigma (\\hat{x}_{uij}) - \\lambda_\\Theta \\frac{\\partial}{\\partial\\Theta} || \\Theta ||^2\\\\\n",
        "      &=  \\sum_{(u,i,j) \\in D_S} \\frac{-e^{-\\hat{x}_{uij}}}{1+e^{-\\hat{x}_{uij}}} \\frac{\\partial}{\\partial \\Theta}\\hat{x}_{uij} - \\lambda_\\Theta \\Theta\n",
        "\\end{split}\n",
        "\\end{equation} $$\n",
        "\n",
        "In order to practically apply this learning schema to an existing algorithm, we first split the real valued preference term: $\\hat{x}_{uij} := \\hat{x}_{ui} − \\hat{x}_{uj}$. And now we can apply any standard collaborative filtering model that predicts $\\hat{x}_{ui}$.\n",
        "\n",
        "The problem of predicting $\\hat{x}_{ui}$ can be seen as the task of estimating a matrix $X:U×I$. With matrix factorization the target matrix $X$ is approximated by the matrix product of two low-rank matrices $W:|U|×k$ and $H:|I|×k$:\n",
        "\n",
        "$$X := WH^t$$\n",
        "\n",
        "The prediction formula can also be written as:\n",
        "\n",
        "$$\\hat{x}_{ui} = \\langle w_u,h_i \\rangle = \\sum_{f=1}^k w_{uf} \\cdot h_{if}$$\n",
        "\n",
        "Besides the dot product ⟨⋅,⋅⟩, in general any kernel can be used.\n",
        "\n",
        "We can now specify the derivatives:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial \\theta} \\hat{x}_{uij} = \\begin{cases}\n",
        "(h_{if} - h_{jf}) \\text{ if } \\theta=w_{uf}, \\\\\n",
        "w_{uf} \\text{ if } \\theta = h_{if}, \\\\\n",
        "-w_{uf} \\text{ if } \\theta = h_{jf}, \\\\\n",
        "0 \\text{ else }\n",
        "\\end{cases}$$\n",
        "\n",
        "Which basically means: user $u$ prefer $i$ over $j$, let's do the following:\n",
        "\n",
        "- Increase the relevance (according to $u$) of features belonging to $i$ but not to $j$ and vice-versa\n",
        "- Increase the relevance of features assigned to $i$\n",
        "- Decrease the relevance of features assigned to $j$\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class BPR(Model):\n",
        "    def __init__(self, feature_columns, mode='inner', embed_reg=1e-6):\n",
        "        \"\"\"\n",
        "        BPR\n",
        "        :param feature_columns: A list. user feature columns + item feature columns\n",
        "        :mode: A string. 'inner' or 'dist'.\n",
        "        :param embed_reg: A scalar.  The regularizer of embedding.\n",
        "        \"\"\"\n",
        "        super(BPR, self).__init__()\n",
        "        # feature columns\n",
        "        self.user_fea_col, self.item_fea_col = feature_columns\n",
        "        # mode\n",
        "        self.mode = mode\n",
        "        # user embedding\n",
        "        self.user_embedding = Embedding(input_dim=self.user_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.user_fea_col['embed_dim'],\n",
        "                                        mask_zero=False,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        # item embedding\n",
        "        self.item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.item_fea_col['embed_dim'],\n",
        "                                        mask_zero=True,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_inputs, pos_inputs, neg_inputs = inputs  # (None, 1), (None, 1)\n",
        "        # user info\n",
        "        user_embed = self.user_embedding(user_inputs)  # (None, 1, dim)\n",
        "        # item\n",
        "        pos_embed = self.item_embedding(pos_inputs)  # (None, 1, dim)\n",
        "        neg_embed = self.item_embedding(neg_inputs)  # (None, 1, dim)\n",
        "        if self.mode == 'inner':\n",
        "            # calculate positive item scores and negative item scores\n",
        "            pos_scores = tf.reduce_sum(tf.multiply(user_embed, pos_embed), axis=-1)  # (None, 1)\n",
        "            neg_scores = tf.reduce_sum(tf.multiply(user_embed, neg_embed), axis=-1)  # (None, 1)\n",
        "            # add loss. Computes softplus: log(exp(features) + 1)\n",
        "            # self.add_loss(tf.reduce_mean(tf.math.softplus(neg_scores - pos_scores)))\n",
        "1➡️        self.add_loss(tf.reduce_mean(-tf.math.log(tf.nn.sigmoid(pos_scores - neg_scores))))\n",
        "        else:\n",
        "            # clip by norm\n",
        "            # user_embed = tf.clip_by_norm(user_embed, 1, -1)\n",
        "            # pos_embed = tf.clip_by_norm(pos_embed, 1, -1)\n",
        "            # neg_embed = tf.clip_by_norm(neg_embed, 1, -1)\n",
        "            pos_scores = tf.reduce_sum(tf.square(user_embed - pos_embed), axis=-1)\n",
        "            neg_scores = tf.reduce_sum(tf.square(user_embed - neg_embed), axis=-1)\n",
        "            self.add_loss(tf.reduce_sum(tf.nn.relu(pos_scores - neg_scores + 0.5)))\n",
        "        logits = tf.concat([pos_scores, neg_scores], axis=-1)\n",
        "        return logits\n",
        "\n",
        "    def summary(self):\n",
        "        user_inputs = Input(shape=(1, ), dtype=tf.int32)\n",
        "        pos_inputs = Input(shape=(1, ), dtype=tf.int32)\n",
        "        neg_inputs = Input(shape=(1, ), dtype=tf.int32)\n",
        "        Model(inputs=[user_inputs, pos_inputs, neg_inputs],\n",
        "            outputs=self.call([user_inputs, pos_inputs, neg_inputs])).summary()\n",
        "```\n",
        "\n",
        "1➡️ To calculate loss, we are first taking the sigmoid of the difference between positive and negative pairs, and then taking mean of the log of these sigmoid values. This is same as equation (4), without regularization term.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/cd6171c364ecc51a912cc0d50fa3ccdc) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Neural Collaborative Filtering (NCF)\n",
        "\n",
        "NCF is a deep learning-based framework for making recommendations. The key idea is to learn the user-item interaction using neural networks. Despite the effectiveness of MF for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction function — the inner product. The hypothesis is that the inner product, which simply combines the multiplication of latent features linearly, may not be sufficient to capture the complex structure of user interaction data. NCF tries to learn this interaction function from data. \n",
        "\n",
        "![[Source](https://d2l.ai/chapter_recommender-systems/neumf.html)](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img1.png)\n",
        "\n",
        "[Source](https://d2l.ai/chapter_recommender-systems/neumf.html)\n",
        "\n",
        "Two instantiations of NCF are Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP). GMF applies a linear kernel to model the latent feature interactions, and MLP uses a nonlinear kernel to learn the interaction function from data. NeuMF is a fused model of GMF and MLP to better model the complex user-item interactions and unifies the strengths of linearity of MF and non-linearity of MLP for modeling the user-item latent structures. NeuMF allows GMF and MLP to learn separate embeddings and combines the two models by concatenating their last hidden layer.\n",
        "\n",
        "The interaction matrix is based on implicit data and contains only 1 or 0. Here a value of 1 indicates that there is an interaction between user u and item i; however, it does not mean u actually likes i. Similarly, a value of 0 does not necessarily mean u does not like i, it can be that the user is not aware of the item. This poses challenges in learning from implicit data since it provides only noisy signals about users’ preferences. While observed entries at least reflect users’ interest in items, the unobserved entries can be just missing data and there is a natural scarcity of negative feedback.\n",
        "\n",
        "The loss function can either be pointwise or pairwise. Due to the non-convexity of the objective function of NeuMF, gradient-based optimization methods only find locally optimal solutions. It is reported that initialization plays an important role in the convergence and performance of deep learning models. Since NeuMF is an ensemble of GMF and MLP, we usually initialize NeuMF using the pre-trained models of GMF and MLP.\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class DNN(Layer):\n",
        "\t\"\"\"\n",
        "\tDeep part\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, hidden_units, activation='relu', dnn_dropout=0., **kwargs):\n",
        "\t\t\"\"\"\n",
        "\t\tDNN part\n",
        "\t\t:param hidden_units: A list. List of hidden layer units's numbers\n",
        "\t\t:param activation: A string. Activation function\n",
        "\t\t:param dnn_dropout: A scalar. dropout number\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(DNN, self).__init__(**kwargs)\n",
        "\t\tself.dnn_network = [Dense(units=unit, activation=activation) for unit in hidden_units]\n",
        "\t\tself.dropout = Dropout(dnn_dropout)\n",
        "\n",
        "\tdef call(self, inputs, **kwargs):\n",
        "\t\tx = inputs\n",
        "\t\tfor dnn in self.dnn_network:\n",
        "1➡️\t\tx = dnn(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\treturn x\n",
        "```\n",
        "\n",
        "```python\n",
        "class NCF(Model):\n",
        "    def __init__(self, feature_columns, hidden_units=None, dropout=0.2, activation='relu', embed_reg=1e-6, **kwargs):\n",
        "        \"\"\"\n",
        "        NCF model\n",
        "        :param feature_columns: A list. user feature columns + item feature columns\n",
        "        :param hidden_units: A list.\n",
        "        :param dropout: A scalar.\n",
        "        :param activation: A string.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        \"\"\"\n",
        "        super(NCF, self).__init__(**kwargs)\n",
        "        if hidden_units is None:\n",
        "            hidden_units = [64, 32, 16, 8]\n",
        "        # feature columns\n",
        "        self.user_fea_col, self.item_fea_col = feature_columns\n",
        "        # MF user embedding\n",
        "        self.mf_user_embedding = Embedding(input_dim=self.user_fea_col['feat_num'],\n",
        "                                           input_length=1,\n",
        "                                           output_dim=self.user_fea_col['embed_dim'],\n",
        "                                           embeddings_initializer='random_normal',\n",
        "                                           embeddings_regularizer=l2(embed_reg))\n",
        "        # MF item embedding\n",
        "        self.mf_item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                           input_length=1,\n",
        "                                           output_dim=self.item_fea_col['embed_dim'],\n",
        "                                           embeddings_initializer='random_normal',\n",
        "                                           embeddings_regularizer=l2(embed_reg))\n",
        "        # MLP user embedding\n",
        "        self.mlp_user_embedding = Embedding(input_dim=self.user_fea_col['feat_num'],\n",
        "                                            input_length=1,\n",
        "                                            output_dim=self.user_fea_col['embed_dim'],\n",
        "                                            embeddings_initializer='random_normal',\n",
        "                                            embeddings_regularizer=l2(embed_reg))\n",
        "        # MLP item embedding\n",
        "        self.mlp_item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                            input_length=1,\n",
        "                                            output_dim=self.item_fea_col['embed_dim'],\n",
        "                                            embeddings_initializer='random_normal',\n",
        "                                            embeddings_regularizer=l2(embed_reg))\n",
        "        # dnn\n",
        "        self.dnn = DNN(hidden_units, activation=activation, dnn_dropout=dropout)\n",
        "        self.dense = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_inputs, pos_inputs, neg_inputs = inputs  # (None, 1), (None, 1), (None, 1/101)\n",
        "        # user info\n",
        "        mf_user_embed = self.mf_user_embedding(user_inputs)  # (None, 1, dim)\n",
        "        mlp_user_embed = self.mlp_user_embedding(user_inputs)  # (None, 1, dim)\n",
        "        # item\n",
        "        mf_pos_embed = self.mf_item_embedding(pos_inputs)  # (None, 1, dim)\n",
        "        mf_neg_embed = self.mf_item_embedding(neg_inputs)  # (None, 1/101, dim)\n",
        "        mlp_pos_embed = self.mlp_item_embedding(pos_inputs)  # (None, 1, dim)\n",
        "        mlp_neg_embed = self.mlp_item_embedding(neg_inputs)  # (None, 1/101, dim)\n",
        "        # MF\n",
        "2➡️    mf_pos_vector = tf.nn.sigmoid(tf.multiply(mf_user_embed, mf_pos_embed))  # (None, 1, dim)\n",
        "        mf_neg_vector = tf.nn.sigmoid(tf.multiply(mf_user_embed, mf_neg_embed))  # (None, 1, dim)\n",
        "        # MLP\n",
        "3➡️    mlp_pos_vector = tf.concat([mlp_user_embed, mlp_pos_embed], axis=-1)  # (None, 1, 2 * dim)\n",
        "        mlp_neg_vector = tf.concat([tf.tile(mlp_user_embed, multiples=[1, mlp_neg_embed.shape[1], 1]),\n",
        "                                    mlp_neg_embed], axis=-1)  # (None, 1/101, 2 * dim)\n",
        "        mlp_pos_vector = self.dnn(mlp_pos_vector)  # (None, 1, dim)\n",
        "        mlp_neg_vector = self.dnn(mlp_neg_vector)  # (None, 1/101, dim)\n",
        "        # concat\n",
        "        pos_vector = tf.concat([mf_pos_vector, mlp_pos_vector], axis=-1)  # (None, 1, 2 * dim)\n",
        "        neg_vector = tf.concat([mf_neg_vector, mlp_neg_vector], axis=-1)  # (None, 1/101, 2 * dim)\n",
        "        # pos_vector = mlp_pos_vector\n",
        "        # neg_vector = mlp_neg_vector\n",
        "        # result\n",
        "        pos_logits = tf.squeeze(self.dense(pos_vector), axis=-1)  # (None, 1)\n",
        "        neg_logits = tf.squeeze(self.dense(neg_vector), axis=-1)  # (None, 1/101)\n",
        "        # loss\n",
        "4➡️    losses = tf.reduce_mean(- tf.math.log(tf.nn.sigmoid(pos_logits)) -\n",
        "                                tf.math.log(1 - tf.nn.sigmoid(neg_logits))) / 2\n",
        "        self.add_loss(losses)\n",
        "        logits = tf.concat([pos_logits, neg_logits], axis=-1)\n",
        "        return logits\n",
        "\n",
        "    def summary(self):\n",
        "        user_inputs = Input(shape=(1,), dtype=tf.int32)\n",
        "        pos_inputs = Input(shape=(1,), dtype=tf.int32)\n",
        "        neg_inputs = Input(shape=(1,), dtype=tf.int32)\n",
        "        Model(inputs=[user_inputs, pos_inputs, neg_inputs],\n",
        "              outputs=self.call([user_inputs, pos_inputs, neg_inputs])).summary()\n",
        "```\n",
        "\n",
        "1➡️ In `self.dnn_network`, we created a list of Dense layers as per the given list of hidden nodes. Now, we are stitching them up in a linear fashion, passing the output of first hidden layer to the next one.\n",
        "\n",
        "2➡️ In MF part, we are multiplying the user and positive (resp. negative) item embeddings.\n",
        "\n",
        "3➡️ In MLP part, we are concatenating these embeddings.\n",
        "\n",
        "4➡️ We are calculating cross-entropy loss.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/1b43e0adba59114598f18dcd9ad239b0) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Convolutional Sequence Embedding Recommendation (Caser)\n",
        "\n",
        "Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a 'near future'. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. Convolutional Sequence Embedding Recommendation Model (Caser) address this requirement by embedding a sequence of recent items into an image' in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns.\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img2.png](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img2.png)\n",
        "\n",
        "Caser adopts convolutional neural networks capture the dynamic pattern influences of users’ recent activities. The main component of Caser consists of a horizontal convolutional network and a vertical convolutional network, aiming to uncover the union-level and point-level sequence patterns, respectively. Point-level pattern indicates the impact of single item in the historical sequence on the target item, while union level pattern implies the influences of several previous actions on the subsequent target. For example, buying both milk and butter together leads to higher probability of buying flour than just buying one of them. Moreover, users’ general interests, or long term preferences are also modeled in the last fully-connected layers, resulting in a more comprehensive modeling of user interests.\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class Caser(Model):\n",
        "    def __init__(self, feature_columns, maxlen=40, hor_n=2, hor_h=8, ver_n=8, dropout=0.5, activation='relu', embed_reg=1e-6):\n",
        "        \"\"\"\n",
        "        AttRec\n",
        "        :param feature_columns: A feature columns list. user + seq\n",
        "        :param maxlen: A scalar. In the paper, maxlen is L, the number of latest items.\n",
        "        :param hor_n: A scalar. The number of horizontal filters.\n",
        "        :param hor_h: A scalar. Height of horizontal filters.\n",
        "        :param ver_n: A scalar. The number of vertical filters.\n",
        "        :param dropout: A scalar. The number of dropout.\n",
        "        :param activation: A string. 'relu', 'sigmoid' or 'tanh'.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        \"\"\"\n",
        "        super(Caser, self).__init__()\n",
        "        # maxlen\n",
        "        self.maxlen = maxlen\n",
        "        # feature columns\n",
        "        self.user_fea_col, self.item_fea_col = feature_columns\n",
        "        # embed_dim\n",
        "        self.embed_dim = self.item_fea_col['embed_dim']\n",
        "        # total number of item set\n",
        "        self.total_item = self.item_fea_col['feat_num']\n",
        "        # horizontal filters\n",
        "        self.hor_n = hor_n\n",
        "        self.hor_h = hor_h if hor_h <= self.maxlen else self.maxlen\n",
        "        # vertical filters\n",
        "        self.ver_n = ver_n\n",
        "        self.ver_w = 1\n",
        "        # user embedding\n",
        "        self.user_embedding = Embedding(input_dim=self.user_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.user_fea_col['embed_dim'],\n",
        "                                        mask_zero=False,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        # item embedding\n",
        "        self.item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.item_fea_col['embed_dim'],\n",
        "                                        mask_zero=True,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        # item2 embedding\n",
        "        self.item2_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.item_fea_col['embed_dim'] * 2,\n",
        "                                        mask_zero=True,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        # horizontal conv\n",
        "        self.hor_conv = Conv1D(filters=self.hor_n, kernel_size=self.hor_h)\n",
        "        # vertical conv, should transpose\n",
        "        self.ver_conv = Conv1D(filters=self.ver_n, kernel_size=self.ver_w)\n",
        "        # max_pooling\n",
        "        self.pooling = GlobalMaxPooling1D()\n",
        "        # dense\n",
        "        self.dense = Dense(self.embed_dim, activation=activation)\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input\n",
        "        user_inputs, seq_inputs, item_inputs = inputs\n",
        "        # user info\n",
        "        user_embed = self.user_embedding(tf.squeeze(user_inputs, axis=-1))  # (None, dim)\n",
        "        # seq info\n",
        "        seq_embed = self.item_embedding(seq_inputs)  # (None, maxlen, dim)\n",
        "        # horizontal conv (None, (maxlen - kernel_size + 2 * pad) / stride +1, hor_n)\n",
        "        hor_info = self.hor_conv(seq_embed)\n",
        "        hor_info = self.pooling(hor_info)  # (None, hor_n)\n",
        "        # vertical conv  (None, (dim - 1 + 2 * pad) / stride + 1, ver_n)\n",
        "        ver_info = self.ver_conv(tf.transpose(seq_embed, perm=(0, 2, 1)))\n",
        "        ver_info = tf.reshape(ver_info, shape=(-1, ver_info.shape[1] * ver_info.shape[2]))  # (None, ?)\n",
        "        # info\n",
        "        seq_info = self.dense(tf.concat([hor_info, ver_info], axis=-1))  # (None, d)\n",
        "1➡️    seq_info = self.dropout(seq_info)\n",
        "        # concat\n",
        "2➡️     info = tf.concat([seq_info, user_embed], axis=-1)  # (None, 2 * d)\n",
        "        # item info\n",
        "        item_embed = self.item2_embedding(tf.squeeze(item_inputs, axis=-1))  # (None, dim)\n",
        "        # predict\n",
        "        outputs = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(info, item_embed), axis=1, keepdims=True))\n",
        "        return outputs\n",
        "\n",
        "    def summary(self):\n",
        "        seq_inputs = Input(shape=(self.maxlen,), dtype=tf.int32)\n",
        "        user_inputs = Input(shape=(1, ), dtype=tf.int32)\n",
        "        item_inputs = Input(shape=(1,), dtype=tf.int32)\n",
        "        Model(inputs=[user_inputs, seq_inputs, item_inputs],\n",
        "              outputs=self.call([user_inputs, seq_inputs, item_inputs])).summary()\n",
        "```\n",
        "\n",
        "1➡️ We applied the horizontal and vertical convolution filter on the sequence data and concatenated the embeddings. Now, we are applying dense layer and a dropout layer.\n",
        "\n",
        "2➡️ We concatenated this sequence embedding with user embeddings to get the final user vector. It is now assumed that the whole user behaviour is being summarized with this embedding.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/79b9cba4161439e59caec63bbcd9fe69) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Self-Attentive Sequential Recommendation (SASRec)\n",
        "\n",
        "Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the ‘context’ of users’ activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user’s next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. SASRec captures the long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC).\n",
        "\n",
        "![US512148 _ General Recommenders-L186674 _ SASRec Model.drawio.png](Matching%20and%20Ranking%20models%20in%20Tensorflow%20f11d7ed7ef8d4838b5ae344cad0fae44/US512148___General_Recommenders-L186674___SASRec_Model.drawio.png)\n",
        "\n",
        "At each time step, SASRec seeks to identify which items are ‘relevant’ from a user’s action history, and use them to predict the next item. Extensive empirical studies show that this method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models.\n",
        "\n",
        "We adopt the binary cross entropy loss as the objective function:\n",
        "\n",
        "$$-\\sum_{S^u\\in S} \\sum_{t \\in [1,2,\\dots,n]}\\left[ log(\\sigma(r_{o_t,t})) + \\sum_{j \\notin S^u} log(1-\\sigma(r_{j,t})) \\right]$$\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class FFN(Layer):\n",
        "    def __init__(self, hidden_unit, d_model):\n",
        "        \"\"\"\n",
        "        Feed Forward Network\n",
        "        :param hidden_unit: A scalar. W1\n",
        "        :param d_model: A scalar. W2\n",
        "        \"\"\"\n",
        "        super(FFN, self).__init__()\n",
        "        self.conv1 = Conv1D(filters=hidden_unit, kernel_size=1, activation='relu', use_bias=True)\n",
        "        self.conv2 = Conv1D(filters=d_model, kernel_size=1, activation=None, use_bias=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "1➡️    output = self.conv2(x)\n",
        "        return output\n",
        "\n",
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, d_model, num_heads=1, ffn_hidden_unit=128, dropout=0., norm_training=True, causality=True):\n",
        "        \"\"\"\n",
        "        Encoder Layer\n",
        "        :param d_model: A scalar. The self-attention hidden size.\n",
        "        :param num_heads: A scalar. Number of heads.\n",
        "        :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n",
        "        :param dropout: A scalar. Number of dropout.\n",
        "        :param norm_training: Boolean. If True, using layer normalization, default True\n",
        "        :param causality: Boolean. If True, using causality, default True\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, causality)\n",
        "        self.ffn = FFN(ffn_hidden_unit, d_model)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6, trainable=norm_training)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6, trainable=norm_training)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, mask = inputs\n",
        "        # self-attention\n",
        "        att_out = self.mha(x, x, x, mask)  # （None, seq_len, d_model)\n",
        "        att_out = self.dropout1(att_out)\n",
        "        # residual add\n",
        "        out1 = self.layernorm1(x + att_out)\n",
        "        # ffn\n",
        "        ffn_out = self.ffn(out1)\n",
        "        ffn_out = self.dropout2(ffn_out)\n",
        "        # residual add\n",
        "  2➡️  out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n",
        "        return out2\n",
        "```\n",
        "\n",
        "```python\n",
        "class SASRec(tf.keras.Model):\n",
        "    def __init__(self, item_fea_col, blocks=1, num_heads=1, ffn_hidden_unit=128,\n",
        "                 dropout=0., maxlen=40, norm_training=True, causality=False, embed_reg=1e-6):\n",
        "        \"\"\"\n",
        "        SASRec model\n",
        "        :param item_fea_col: A dict contains 'feat_name', 'feat_num' and 'embed_dim'.\n",
        "        :param blocks: A scalar. The Number of blocks.\n",
        "        :param num_heads: A scalar. Number of heads.\n",
        "        :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n",
        "        :param dropout: A scalar. Number of dropout.\n",
        "        :param maxlen: A scalar. Number of length of sequence\n",
        "        :param norm_training: Boolean. If True, using layer normalization, default True\n",
        "        :param causality: Boolean. If True, using causality, default True\n",
        "        :param embed_reg: A scalar. The regularizer of embedding\n",
        "        \"\"\"\n",
        "        super(SASRec, self).__init__()\n",
        "        # sequence length\n",
        "        self.maxlen = maxlen\n",
        "        # item feature columns\n",
        "        self.item_fea_col = item_fea_col\n",
        "        # embed_dim\n",
        "        self.embed_dim = self.item_fea_col['embed_dim']\n",
        "        # d_model must be the same as embedding_dim, because of residual connection\n",
        "        self.d_model = self.embed_dim\n",
        "        # item embedding\n",
        "        self.item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.item_fea_col['embed_dim'],\n",
        "                                        mask_zero=True,\n",
        "                                        embeddings_initializer='random_uniform',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        self.pos_embedding = Embedding(input_dim=self.maxlen,\n",
        "                                       input_length=1,\n",
        "                                       output_dim=self.embed_dim,\n",
        "                                       mask_zero=False,\n",
        "                                       embeddings_initializer='random_uniform',\n",
        "                                       embeddings_regularizer=l2(embed_reg))\n",
        "        self.dropout = Dropout(dropout)\n",
        "        # attention block\n",
        "3➡️    self.encoder_layer = [EncoderLayer(self.d_model, num_heads, ffn_hidden_unit,\n",
        "                                           dropout, norm_training, causality) for b in range(blocks)]\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs\n",
        "        seq_inputs, pos_inputs, neg_inputs = inputs  # (None, maxlen), (None, 1), (None, 1)\n",
        "        # mask\n",
        "        mask = tf.expand_dims(tf.cast(tf.not_equal(seq_inputs, 0), dtype=tf.float32), axis=-1)  # (None, maxlen, 1)\n",
        "        # seq info\n",
        "        seq_embed = self.item_embedding(seq_inputs)  # (None, maxlen, dim)\n",
        "        # pos encoding\n",
        "        # pos_encoding = positional_encoding(seq_inputs, self.embed_dim)\n",
        "        pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.maxlen)), axis=0)\n",
        "        seq_embed += pos_encoding\n",
        "        seq_embed = self.dropout(seq_embed)\n",
        "        att_outputs = seq_embed  # (None, maxlen, dim)\n",
        "        att_outputs *= mask\n",
        "\n",
        "        # self-attention\n",
        "        for block in self.encoder_layer:\n",
        "            att_outputs = block([att_outputs, mask])  # (None, seq_len, dim)\n",
        "            att_outputs *= mask\n",
        "\n",
        "        # user_info = tf.reduce_mean(att_outputs, axis=1)  # (None, dim)\n",
        "4➡️     user_info = tf.expand_dims(att_outputs[:, -1], axis=1)  # (None, 1, dim)\n",
        "        # item info\n",
        "        pos_info = self.item_embedding(pos_inputs)  # (None, 1, dim)\n",
        "        neg_info = self.item_embedding(neg_inputs)  # (None, 1/100, dim)\n",
        "        pos_logits = tf.reduce_sum(user_info * pos_info, axis=-1)  # (None, 1)\n",
        "        neg_logits = tf.reduce_sum(user_info * neg_info, axis=-1)  # (None, 1)\n",
        "        # loss\n",
        "5➡️    losses = tf.reduce_mean(- tf.math.log(tf.nn.sigmoid(pos_logits)) -\n",
        "                                tf.math.log(1 - tf.nn.sigmoid(neg_logits))) / 2\n",
        "        self.add_loss(losses)\n",
        "        logits = tf.concat([pos_logits, neg_logits], axis=-1)\n",
        "        return logits\n",
        "\n",
        "    def summary(self):\n",
        "        seq_inputs = Input(shape=(self.maxlen,), dtype=tf.int32)\n",
        "        pos_inputs = Input(shape=(1,), dtype=tf.int32)\n",
        "        neg_inputs = Input(shape=(1,), dtype=tf.int32)\n",
        "        tf.keras.Model(inputs=[seq_inputs, pos_inputs, neg_inputs],\n",
        "                       outputs=self.call([seq_inputs, pos_inputs, neg_inputs])).summary()\n",
        "```\n",
        "\n",
        "1➡️ FFN layer consists of two 1-d convolution layers stitched in a linear fashion.\n",
        "\n",
        "2➡️ Encoder layer consists of encoded output from multi-head attention layer, which is being passed to FFN layer.\n",
        "\n",
        "3➡️ `self.encoder_layer` is a list of encoder blocks. blocks=1 would mean single block only.\n",
        "\n",
        "4➡️ Final user vector is being extracted from the last layer of encoder blocks.\n",
        "\n",
        "5➡️ We are using cross-entropy loss.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/43cc1b4102c9b9f9c93a1c1289057746) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Self-Attentive Sequential Recommendation\n",
        "\n",
        "The loss function can either be pointwise or pairwise. Due to the non-convexity of the objective function of NeuMF, gradient-based optimization methods only find locally optimal solutions. It is reported that initialization plays an important role in the convergence and performance of deep learning models. Since NeuMF is an ensemble of GMF and MLP, we usually initialize NeuMF using the pre-trained models of GMF and MLP.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img3.png)\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class AttRec(Model):\n",
        "    def __init__(self, feature_columns, maxlen=40, mode='inner', gamma=0.5, w=0.5, embed_reg=1e-6, **kwargs):\n",
        "        \"\"\"\n",
        "        AttRec\n",
        "        :param feature_columns: A feature columns list. user + seq\n",
        "        :param maxlen: A scalar. In the paper, maxlen is L, the number of latest items.\n",
        "        :param gamma: A scalar. if mode == 'dist', gamma is the margin.\n",
        "        :param mode: A string. inner or dist.\n",
        "        :param w: A scalar. The weight of short interest.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        \"\"\"\n",
        "        super(AttRec, self).__init__(**kwargs)\n",
        "        # maxlen\n",
        "        self.maxlen = maxlen\n",
        "        # w\n",
        "        self.w = w\n",
        "        self.gamma = gamma\n",
        "        self.mode = mode\n",
        "        # feature columns\n",
        "        self.user_fea_col, self.item_fea_col = feature_columns\n",
        "        # embed_dim\n",
        "        self.embed_dim = self.item_fea_col['embed_dim']\n",
        "        # user embedding\n",
        "        self.user_embedding = Embedding(input_dim=self.user_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.user_fea_col['embed_dim'],\n",
        "                                        mask_zero=False,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        # item embedding\n",
        "        self.item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.item_fea_col['embed_dim'],\n",
        "                                        mask_zero=True,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        # item2 embedding, not share embedding\n",
        "        self.item2_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n",
        "                                        input_length=1,\n",
        "                                        output_dim=self.item_fea_col['embed_dim'],\n",
        "                                        mask_zero=True,\n",
        "                                        embeddings_initializer='random_normal',\n",
        "                                        embeddings_regularizer=l2(embed_reg))\n",
        "        # self-attention\n",
        "        self.self_attention = SelfAttention_Layer()\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # input\n",
        "        user_inputs, seq_inputs, pos_inputs, neg_inputs = inputs\n",
        "        # mask\n",
        "        # mask = self.item_embedding.compute_mask(seq_inputs)\n",
        "        mask = tf.cast(tf.not_equal(seq_inputs, 0), dtype=tf.float32)  # (None, maxlen)\n",
        "        # user info\n",
        "        user_embed = self.user_embedding(tf.squeeze(user_inputs, axis=-1))  # (None, dim)\n",
        "        # seq info\n",
        "        seq_embed = self.item_embedding(seq_inputs)  # (None, maxlen, dim)\n",
        "        # item\n",
        "        pos_embed = self.item_embedding(tf.squeeze(pos_inputs, axis=-1))  # (None, dim)\n",
        "        neg_embed = self.item_embedding(tf.squeeze(neg_inputs, axis=-1))  # (None, dim)\n",
        "        # item2 embed\n",
        "        pos_embed2 = self.item2_embedding(tf.squeeze(pos_inputs, axis=-1))  # (None, dim)\n",
        "        neg_embed2 = self.item2_embedding(tf.squeeze(neg_inputs, axis=-1))  # (None, dim)\n",
        "\n",
        "        # short-term interest\n",
        "1➡️    short_interest = self.self_attention([seq_embed, seq_embed, seq_embed, mask])  # (None, dim)\n",
        "\n",
        "        # mode\n",
        "        if self.mode == 'inner':\n",
        "            # long-term interest, pos and neg\n",
        "2➡️        pos_long_interest = tf.multiply(user_embed, pos_embed2)\n",
        "            neg_long_interest = tf.multiply(user_embed, neg_embed2)\n",
        "            # combine\n",
        "            pos_scores = self.w * tf.reduce_sum(pos_long_interest, axis=-1, keepdims=True) \\\n",
        "                         + (1 - self.w) * tf.reduce_sum(tf.multiply(short_interest, pos_embed), axis=-1, keepdims=True)\n",
        "            neg_scores = self.w * tf.reduce_sum(neg_long_interest, axis=-1, keepdims=True) \\\n",
        "                         + (1 - self.w) * tf.reduce_sum(tf.multiply(short_interest, neg_embed), axis=-1, keepdims=True)\n",
        "            self.add_loss(tf.reduce_mean(-tf.math.log(tf.nn.sigmoid(pos_scores - neg_scores))))\n",
        "        else:\n",
        "            # clip by norm\n",
        "            user_embed = tf.clip_by_norm(user_embed, 1, -1)\n",
        "            pos_embed = tf.clip_by_norm(pos_embed, 1, -1)\n",
        "            neg_embed = tf.clip_by_norm(neg_embed, 1, -1)\n",
        "            pos_embed2 = tf.clip_by_norm(pos_embed2, 1, -1)\n",
        "            neg_embed2 = tf.clip_by_norm(neg_embed2, 1, -1)\n",
        "            # distance\n",
        "            # long-term interest, pos and neg\n",
        "            pos_long_interest = tf.square(user_embed - pos_embed2)  # (None, dim)\n",
        "            neg_long_interest = tf.square(user_embed - neg_embed2)  # (None, dim)\n",
        "            # combine. Here is a difference from the original paper.\n",
        "            pos_scores = self.w * tf.reduce_sum(pos_long_interest, axis=-1, keepdims=True) + \\\n",
        "                         (1 - self.w) * tf.reduce_sum(tf.square(short_interest - pos_embed), axis=-1, keepdims=True)\n",
        "            neg_scores = self.w * tf.reduce_sum(neg_long_interest, axis=-1, keepdims=True) + \\\n",
        "                         (1 - self.w) * tf.reduce_sum(tf.square(short_interest - neg_embed), axis=-1, keepdims=True)\n",
        "            # minimize loss\n",
        "            # self.add_loss(tf.reduce_sum(tf.maximum(pos_scores - neg_scores + self.gamma, 0)))\n",
        "3➡️        self.add_loss(tf.reduce_sum(tf.nn.relu(pos_scores - neg_scores + self.gamma)))\n",
        "        return pos_scores, neg_scores\n",
        "\n",
        "    def summary(self):\n",
        "        seq_inputs = Input(shape=(self.maxlen,), dtype=tf.int32)\n",
        "        user_inputs = Input(shape=(1, ), dtype=tf.int32)\n",
        "        pos_inputs = Input(shape=(1, ), dtype=tf.int32)\n",
        "        neg_inputs = Input(shape=(1, ), dtype=tf.int32)\n",
        "        Model(inputs=[user_inputs, seq_inputs, pos_inputs, neg_inputs], \n",
        "            outputs=self.call([user_inputs, seq_inputs, pos_inputs, neg_inputs])).summary()\n",
        "```\n",
        "\n",
        "1➡️ Short-term interest vector is calculated by encoding the item sequences using self-attention mechanism.\n",
        "\n",
        "2➡️ Long-term interest vector is the vector multiplication of user embedding and positive (resp. negative) item embedding.\n",
        "\n",
        "3➡️ Loss is the sum of euclidean distance loss of both short and long-term interest vectors.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/eb21eee590e13931dac8019cd4dcb576) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "## Ranking models on Criteo (sample) Ad display dataset\n",
        "\n",
        "### 1. Factorization Machines (FM)\n",
        "\n",
        "Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Factorization Machine type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data.\n",
        "\n",
        "Factorization machines (FM) [Rendle, 2010], proposed by Steffen Rendle in 2010, is a supervised algorithm that can be used for classification, regression, and ranking tasks. It quickly took notice and became a popular and impactful method for making predictions and recommendations. Particularly, it is a generalization of the linear regression model and the matrix factorization model. Moreover, it is reminiscent of support vector machines with a polynomial kernel. The strengths of factorization machines over the linear regression and matrix factorization are: (1) it can model χ -way variable interactions, where χ is the number of polynomial order and is usually set to two. (2) A fast optimization algorithm associated with factorization machines can reduce the polynomial computation time to linear complexity, making it extremely efficient especially for high dimensional sparse inputs. For these reasons, factorization machines are widely employed in modern advertisement and products recommendations.\n",
        "\n",
        "Most recommendation problems assume that we have a consumption/rating dataset formed by a collection of *(user, item, rating*) tuples. This is the starting point for most variations of Collaborative Filtering algorithms and they have proven to yield nice results; however, in many applications, we have plenty of item metadata (tags, categories, genres) that can be used to make better predictions. This is one of the benefits of using Factorization Machines with feature-rich datasets, for which there is a natural way in which extra features can be included in the model and higher-order interactions can be modeled using the dimensionality parameter d. For sparse datasets, a second-order FM model suffices, since there is not enough information to estimate more complex interactions.\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img4.png](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img4.png)\n",
        "\n",
        "$$f(x) = w_0 + \\sum_{p=1}^Pw_px_p + \\sum_{p=1}^{P-1}\\sum_{q=p+1}^Pw_{p,q}x_px_q$$\n",
        "\n",
        "This model formulation may look familiar — it's simply a quadratic linear regression. However, unlike polynomial linear models which estimate each interaction term separately, FMs instead use factorized interaction parameters: feature interaction weights are represented as the inner product of the two features' latent factor space embeddings:\n",
        "\n",
        "$$f(x) = w_0 + \\sum_{p=1}^Pw_px_p + \\sum_{p=1}^{P-1}\\sum_{q=p+1}^P\\langle v_p,v_q \\rangle x_px_q$$\n",
        "\n",
        "This greatly decreases the number of parameters to estimate while at the same time facilitating more accurate estimation by breaking the strict independence criteria between interaction terms. Consider a realistic recommendation data set with 1,000,000 users and 10,000 items. A quadratic linear model would need to estimate U + I + UI ~ 10 billion parameters. A FM model of dimension F=10 would need only U + I + F(U + I) ~ 11 million parameters. Additionally, many common MF algorithms (including SVD++, ALS) can be re-formulated as special cases of the more general/flexible FM model class.\n",
        "\n",
        "The above equation can be rewritten as:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n \\hat{w}_{ij} x_{i} x_{j}\n",
        "\\end{align*}$$\n",
        "\n",
        "where,\n",
        "\n",
        "- $w_0$ is the global bias\n",
        "- $w_i$ denotes the weight of the i-th feature,\n",
        "- $\\hat{w}_{ij} = v_i^Tv_j$ denotes the weight of the cross feature $x_ix_j$\n",
        "- $v_i \\in \\mathcal{R}^k$ denotes the embedding vector for feature $i$\n",
        "- $k$ denotes the size of embedding vector\n",
        "\n",
        "<aside>\n",
        "💡 For large, sparse datasets...FM and FFM is good. But for small, dense datasets...try to avoid.\n",
        "\n",
        "</aside>\n",
        "\n",
        "Factorization machines appeared to be the method which answered the challenge!\n",
        "\n",
        "|  | Accuracy | Speed | Sparsity |\n",
        "| --- | --- | --- | --- |\n",
        "| Collaborative Filtering | Too Accurate | Suitable | Suitable |\n",
        "| SVM | Too Accurate | Suitable | Unsuitable |\n",
        "| Random Forest/CART | General Accuracy | Unsuitable | Unsuitable |\n",
        "| Factorization Machines (FM) | General Accuracy | Quick | Designed for it |\n",
        "\n",
        "To learn the FM model, we can use the MSE loss for regression task, the cross entropy loss for classification tasks, and the BPR loss for ranking task. Standard optimizers such as SGD and Adam are viable for optimization.\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class FM_Layer(Layer):\n",
        "    def __init__(self, feature_columns, k, w_reg=1e-6, v_reg=1e-6):\n",
        "        \"\"\"\n",
        "        Factorization Machines\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param k: the latent vector\n",
        "        :param w_reg: the regularization coefficient of parameter w\n",
        "        :param v_reg: the regularization coefficient of parameter v\n",
        "        \"\"\"\n",
        "        super(FM_Layer, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.index_mapping = []\n",
        "        self.feature_length = 0\n",
        "        for feat in self.sparse_feature_columns:\n",
        "            self.index_mapping.append(self.feature_length)\n",
        "            self.feature_length += feat['feat_num']\n",
        "        self.k = k\n",
        "        self.w_reg = w_reg\n",
        "        self.v_reg = v_reg\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w0 = self.add_weight(name='w0', shape=(1,),\n",
        "                                  initializer=tf.zeros_initializer(),\n",
        "                                  trainable=True)\n",
        "        self.w = self.add_weight(name='w', shape=(self.feature_length, 1),\n",
        "                                 initializer=tf.random_normal_initializer(),\n",
        "                                 regularizer=l2(self.w_reg),\n",
        "                                 trainable=True)\n",
        "        self.V = self.add_weight(name='V', shape=(self.feature_length, self.k),\n",
        "                                 initializer=tf.random_normal_initializer(),\n",
        "                                 regularizer=l2(self.v_reg),\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # mapping\n",
        "        inputs = inputs + tf.convert_to_tensor(self.index_mapping)\n",
        "        # first order\n",
        "1➡️    first_order = self.w0 + tf.reduce_sum(tf.nn.embedding_lookup(self.w, inputs), axis=1)  # (batch_size, 1)\n",
        "        # second order\n",
        "        second_inputs = tf.nn.embedding_lookup(self.V, inputs)  # (batch_size, fields, embed_dim)\n",
        "        square_sum = tf.square(tf.reduce_sum(second_inputs, axis=1, keepdims=True))  # (batch_size, 1, embed_dim)\n",
        "        sum_square = tf.reduce_sum(tf.square(second_inputs), axis=1, keepdims=True)  # (batch_size, 1, embed_dim)\n",
        "2➡️    second_order = 0.5 * tf.reduce_sum(square_sum - sum_square, axis=2)  # (batch_size, 1)\n",
        "        # outputs\n",
        "        outputs = first_order + second_order\n",
        "        return outputs\n",
        "\n",
        "class FM(Model):\n",
        "    def __init__(self, feature_columns, k, w_reg=1e-6, v_reg=1e-6):\n",
        "        \"\"\"\n",
        "        Factorization Machines\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param k: the latent vector\n",
        "        :param w_reg: the regularization coefficient of parameter w\n",
        "\t\t:param v_reg: the regularization coefficient of parameter v\n",
        "        \"\"\"\n",
        "        super(FM, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.fm = FM_Layer(feature_columns, k, w_reg, v_reg)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        fm_outputs = self.fm(inputs)\n",
        "        outputs = tf.nn.sigmoid(fm_outputs)\n",
        "        return outputs\n",
        "\n",
        "    def summary(self, **kwargs):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ First-order embeddings are in the simple linear-regression style. It contains a bias term and a list of weights.\n",
        "\n",
        "2➡️ Second-order embeddings is a k-length embedding matrix for each field.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/f070e6b7cf99eb2a5b796447c011890e) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Field-aware Factorization Machines (FFM)\n",
        "\n",
        "Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance.\n",
        "\n",
        "|  | For each | Learn |\n",
        "| --- | --- | --- |\n",
        "| Linear | feature | a weight |\n",
        "| Poly | feature pair | a weight |\n",
        "| FM | feature | a latent vector |\n",
        "| FFM | feature | multiple latent vectors |\n",
        "\n",
        "Field-aware factorization machine (FFM) is an extension to FM. It was originally introduced in [2]. The advantage of FFM over FM is that it uses different factorized latent factors for different groups of features. The \"group\" is called \"field\" in the context of FFM. Putting features into fields resolves the issue that the latent factors shared by features that intuitively represent different categories of information may not well generalize the correlation.\n",
        "\n",
        "FFM addresses this issue by splitting the original latent space into smaller latent spaces specific to the fields of the features.\n",
        "\n",
        "$$\\phi(\\pmb{w}, \\pmb{x}) = w_0 + \\sum\\limits_{i=1}^n w_i x_i + \\sum\\limits_{i=1}^n \\sum\\limits_{j=i + 1}^n \\langle \\mathbf{v}_{i, f_{2}} \\cdot \\mathbf{v}_{j, f_{1}} \\rangle x_i x_j$$\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class FFM_Layer(Layer):\n",
        "    def __init__(self, sparse_feature_columns, k, w_reg=1e-6, v_reg=1e-6):\n",
        "        \"\"\"\n",
        "        :param dense_feature_columns: A list. sparse column feature information.\n",
        "        :param k: A scalar. The latent vector\n",
        "        :param w_reg: A scalar. The regularization coefficient of parameter w\n",
        "\t\t:param v_reg: A scalar. The regularization coefficient of parameter v\n",
        "        \"\"\"\n",
        "        super(FFM_Layer, self).__init__()\n",
        "        self.sparse_feature_columns = sparse_feature_columns\n",
        "        self.k = k\n",
        "        self.w_reg = w_reg\n",
        "        self.v_reg = v_reg\n",
        "        self.index_mapping = []\n",
        "        self.feature_length = 0\n",
        "        for feat in self.sparse_feature_columns:\n",
        "            self.index_mapping.append(self.feature_length)\n",
        "            self.feature_length += feat['feat_num']\n",
        "        self.field_num = len(self.sparse_feature_columns)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w0 = self.add_weight(name='w0', shape=(1,),\n",
        "                                  initializer=tf.zeros_initializer(),\n",
        "                                  trainable=True)\n",
        "        self.w = self.add_weight(name='w', shape=(self.feature_length, 1),\n",
        "                                 initializer='random_normal',\n",
        "                                 regularizer=l2(self.w_reg),\n",
        "                                 trainable=True)\n",
        "        self.v = self.add_weight(name='v',\n",
        "                                 shape=(self.feature_length, self.field_num, self.k),\n",
        "                                 initializer='random_normal',\n",
        "                                 regularizer=l2(self.v_reg),\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        inputs = inputs + tf.convert_to_tensor(self.index_mapping)\n",
        "        # first order\n",
        "        first_order = self.w0 + tf.reduce_sum(tf.nn.embedding_lookup(self.w, inputs), axis=1)  # (batch_size, 1)\n",
        "        # field second order\n",
        "        second_order = 0\n",
        "        latent_vector = tf.reduce_sum(tf.nn.embedding_lookup(self.v, inputs), axis=1)  # (batch_size, field_num, k)\n",
        "        for i in range(self.field_num):\n",
        "            for j in range(i+1, self.field_num):\n",
        "1➡️             second_order += tf.reduce_sum(latent_vector[:, i] * latent_vector[:, j], axis=1, keepdims=True)\n",
        "        return first_order + second_order\n",
        "```\n",
        "\n",
        "```python\n",
        "class FFM(Model):\n",
        "    def __init__(self, feature_columns, k, w_reg=1e-6, v_reg=1e-6):\n",
        "        \"\"\"\n",
        "        FFM architecture\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param k: the latent vector\n",
        "        :param w_reg: the regularization coefficient of parameter w\n",
        "\t\t:param field_reg_reg: the regularization coefficient of parameter v\n",
        "        \"\"\"\n",
        "        super(FFM, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.ffm = FFM_Layer(self.sparse_feature_columns, k, w_reg, v_reg)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        ffm_out = self.ffm(inputs)\n",
        "        outputs = tf.nn.sigmoid(ffm_out)\n",
        "        return outputs\n",
        "\n",
        "    def summary(self, **kwargs):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        tf.keras.Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ Second-order features are being calculated as per the last-part of this equation: \n",
        "\n",
        "$$\\phi(\\pmb{w}, \\pmb{x}) = w_0 + \\sum\\limits_{i=1}^n w_i x_i + \\sum\\limits_{i=1}^n \\sum\\limits_{j=i + 1}^n \\langle \\mathbf{v}_{i, f_{2}} \\cdot \\mathbf{v}_{j, f_{1}} \\rangle x_i x_j$$\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/bb4620cf109c34622b5f10fa4e332445) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Wide & Deep Learning (W&D)\n",
        "\n",
        "Wide and Deep Learning Model, proposed by Google, 2016, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img5.png)\n",
        "\n",
        "To understand the concept of deep & wide recommendations, it’s best to think of it as two separate, but collaborating, engines. The wide model, often referred to in the literature as the linear model, memorizes users and their past product choices. Its inputs may consist simply of a user identifier and a product identifier, though other attributes relevant to the pattern (such as time of day) may also be incorporated.\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img6.png](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img6.png)\n",
        "\n",
        "The deep portion of the model, so named as it is a deep neural network, examines the generalizable attributes of a user and their product choices. From these, the model learns the broader characteristics that tend to favor users’ product selections.\n",
        "\n",
        "Together, the wide and deep submodels are trained on historical product selections by individual users to predict future product selections. The end result is a single model capable of calculating the probability with which a user will purchase a given item, given both memorized past choices and generalizations about a user’s preferences. These probabilities form the basis for user-specific product rankings, which can be used for making recommendations.\n",
        "\n",
        "The goal with wide and deep recommenders is to provide the same level of customer intimacy that, for example, our favorite barista does. This model uses explicit and implicit feedback to expand the considerations set for customers. Wide and deep recommenders go beyond simple weighted averaging of customer feedback found in some collaborative filters to balance what is understood about the individual with what is known about similar customers. If done properly, the recommendations make the customer feel understood and this should translate into greater value for both the customer and the business.\n",
        "\n",
        "The intuitive logic of the wide-and-deep recommender belies the complexity of its actual construction. Inputs must be defined separately for each of the wide and deep portions of the model and each must be trained in a coordinated manner to arrive at a single output, but tuned using optimizers specific to the nature of each submodel. Thankfully, the **[Tensorflow DNNLinearCombinedClassifier estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier)** provides a pre-packaged architecture, greatly simplifying the assembly of the overall model.\n",
        "\n",
        "[source: [https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)](https://youtu.be/Xmw9SWJ0L50)\n",
        "\n",
        "source: [https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class Linear(Layer):\n",
        "    def __init__(self, feature_length, w_reg=1e-6):\n",
        "        \"\"\"\n",
        "        Linear Part\n",
        "        :param feature_length: A scalar. The length of features.\n",
        "        :param w_reg: A scalar. The regularization coefficient of parameter w.\n",
        "        \"\"\"\n",
        "        super(Linear, self).__init__()\n",
        "        self.feature_length = feature_length\n",
        "        self.w_reg = w_reg\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(name=\"w\",\n",
        "                                 shape=(self.feature_length, 1),\n",
        "                                 regularizer=l2(self.w_reg),\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        result = tf.reduce_sum(tf.nn.embedding_lookup(self.w, inputs), axis=1)  # (batch_size, 1)\n",
        "        return result\n",
        "\n",
        "class DNN(Layer):\n",
        "    def __init__(self, hidden_units, activation='relu', dropout=0.):\n",
        "        \"\"\"Deep Neural Network\n",
        "\t\t:param hidden_units: A list. Neural network hidden units.\n",
        "\t\t:param activation: A string. Activation function of dnn.\n",
        "\t\t:param dropout: A scalar. Dropout number.\n",
        "\t\t\"\"\"\n",
        "        super(DNN, self).__init__()\n",
        "        self.dnn_network = [Dense(units=unit, activation=activation) for unit in hidden_units]\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        for dnn in self.dnn_network:\n",
        "            x = dnn(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "```python\n",
        "class WideDeep(Model):\n",
        "    def __init__(self, feature_columns, hidden_units, activation='relu',\n",
        "                 dnn_dropout=0., embed_reg=1e-6, w_reg=1e-6):\n",
        "        \"\"\"\n",
        "        Wide&Deep\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param hidden_units: A list. Neural network hidden units.\n",
        "        :param activation: A string. Activation function of dnn.\n",
        "        :param dnn_dropout: A scalar. Dropout of dnn.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        :param w_reg: A scalar. The regularizer of Linear.\n",
        "        \"\"\"\n",
        "        super(WideDeep, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.embed_layers = {\n",
        "            'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "                                         input_length=1,\n",
        "                                         output_dim=feat['embed_dim'],\n",
        "                                         embeddings_initializer='random_uniform',\n",
        "                                         embeddings_regularizer=l2(embed_reg))\n",
        "            for i, feat in enumerate(self.sparse_feature_columns)\n",
        "        }\n",
        "        self.index_mapping = []\n",
        "        self.feature_length = 0\n",
        "        for feat in self.sparse_feature_columns:\n",
        "            self.index_mapping.append(self.feature_length)\n",
        "            self.feature_length += feat['feat_num']\n",
        "        self.dnn_network = DNN(hidden_units, activation, dnn_dropout)\n",
        "        self.linear = Linear(self.feature_length, w_reg=w_reg)\n",
        "        self.final_dense = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        sparse_embed = tf.concat([self.embed_layers['embed_{}'.format(i)](inputs[:, i])\n",
        "                                  for i in range(inputs.shape[1])], axis=-1)\n",
        "        x = sparse_embed  # (batch_size, field * embed_dim)\n",
        "        # Wide\n",
        "        wide_inputs = inputs + tf.convert_to_tensor(self.index_mapping)\n",
        "1➡️    wide_out = self.linear(wide_inputs)\n",
        "        # Deep\n",
        "        deep_out = self.dnn_network(x)\n",
        "        deep_out = self.final_dense(deep_out)\n",
        "        # out\n",
        "2➡️    outputs = tf.nn.sigmoid(0.5 * wide_out + 0.5 * deep_out)\n",
        "        return outputs\n",
        "\n",
        "    def summary(self, **kwargs):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ In wide-part, we are passing the inputs and index mappings of sparse features to the linear layer.\n",
        "\n",
        "2➡️ Final output is the average weight on the wide and deep model outputs.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/966844969941e710da1e204831a37638) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Deep Crossing\n",
        "\n",
        "The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img7.png)\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class Residual_Units(Layer):\n",
        "    \"\"\"\n",
        "    Residual Units\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_unit, dim_stack):\n",
        "        \"\"\"\n",
        "        :param hidden_unit: A list. Neural network hidden units.\n",
        "        :param dim_stack: A scalar. The dimension of inputs unit.\n",
        "        \"\"\"\n",
        "        super(Residual_Units, self).__init__()\n",
        "        self.layer1 = Dense(units=hidden_unit, activation='relu')\n",
        "        self.layer2 = Dense(units=dim_stack, activation=None)\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "1➡️    outputs = self.relu(x + inputs)\n",
        "        return outputs\n",
        "```\n",
        "\n",
        "```python\n",
        "class Deep_Crossing(Model):\n",
        "    def __init__(self, feature_columns, hidden_units, res_dropout=0., embed_reg=1e-6):\n",
        "        \"\"\"\n",
        "        Deep&Crossing\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param hidden_units: A list. Neural network hidden units.\n",
        "        :param res_dropout: A scalar. Dropout of resnet.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        \"\"\"\n",
        "        super(Deep_Crossing, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.embed_layers = {\n",
        "            'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "                                         input_length=1,\n",
        "                                         output_dim=feat['embed_dim'],\n",
        "                                         embeddings_initializer='random_uniform',\n",
        "                                         embeddings_regularizer=l2(embed_reg))\n",
        "            for i, feat in enumerate(self.sparse_feature_columns)\n",
        "        }\n",
        "        # the total length of embedding layers\n",
        "        embed_layers_len = sum([feat['embed_dim'] for feat in self.sparse_feature_columns])\n",
        "        self.res_network = [Residual_Units(unit, embed_layers_len) for unit in hidden_units]\n",
        "        self.res_dropout = Dropout(res_dropout)\n",
        "        self.dense = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        sparse_inputs = inputs\n",
        "        sparse_embed = tf.concat([self.embed_layers['embed_{}'.format(i)](sparse_inputs[:, i])\n",
        "                                  for i in range(sparse_inputs.shape[1])], axis=-1)\n",
        "        r = sparse_embed\n",
        "2➡️    for res in self.res_network:\n",
        "            r = res(r)\n",
        "        r = self.res_dropout(r)\n",
        "        outputs = tf.nn.sigmoid(self.dense(r))\n",
        "        return outputs\n",
        "\n",
        "    def summary(self):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ In residual units, we are applying relu activation on input and dense layer output of these inputs.\n",
        "\n",
        "2➡️ This is how we are stacking multiple residual units in a sequential fashion.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/d0c5c206daea9ffece59d587ceb02e42) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Product-based Neural Network (PNN)\n",
        "\n",
        "PNN uses an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between inter-field categories, and further fully connected layers to explore high-order feature interactions.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img8.png)\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class PNN(Model):\n",
        "    def __init__(self, feature_columns, hidden_units, mode='in', dnn_dropout=0.,\n",
        "                 activation='relu', embed_reg=1e-6, w_z_reg=1e-6, w_p_reg=1e-6, l_b_reg=1e-6):\n",
        "        \"\"\"\n",
        "        Product-based Neural Networks\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param hidden_units: A list. Neural network hidden units.\n",
        "        :param mode: A string. 'in' IPNN or 'out'OPNN.\n",
        "        :param activation: A string. Activation function of dnn.\n",
        "        :param dnn_dropout: A scalar. Dropout of dnn.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        :param w_z_reg: A scalar. The regularizer of w_z_ in product layer\n",
        "        :param w_p_reg: A scalar. The regularizer of w_p in product layer\n",
        "        :param l_b_reg: A scalar. The regularizer of l_b in product layer\n",
        "        \"\"\"\n",
        "        super(PNN, self).__init__()\n",
        "        # inner product or outer product\n",
        "        self.mode = mode\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        # the number of feature fields\n",
        "        self.field_num = len(self.sparse_feature_columns)\n",
        "        self.embed_dim = self.sparse_feature_columns[0]['embed_dim']\n",
        "        # The embedding dimension of each feature field must be the same\n",
        "        self.embed_layers = {\n",
        "            'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "                                         input_length=1,\n",
        "                                         output_dim=feat['embed_dim'],\n",
        "                                         embeddings_initializer='random_uniform',\n",
        "                                         embeddings_regularizer=l2(embed_reg))\n",
        "            for i, feat in enumerate(self.sparse_feature_columns)\n",
        "        }\n",
        "        # parameters\n",
        "        self.w_z = self.add_weight(name='w_z',\n",
        "                                   shape=(self.field_num, self.embed_dim, hidden_units[0]),\n",
        "                                   initializer='random_uniform',\n",
        "                                   regularizer=l2(w_z_reg),\n",
        "                                   trainable=True\n",
        "                                   )\n",
        "        if mode == 'in':\n",
        "            self.w_p = self.add_weight(name='w_p',\n",
        "                                       shape=(self.field_num * (self.field_num - 1) // 2, self.embed_dim,\n",
        "                                              hidden_units[0]),\n",
        "                                       initializer='random_uniform',\n",
        "                                       reguarizer=l2(w_p_reg),\n",
        "                                       trainable=True)\n",
        "        # out\n",
        "        else:\n",
        "            self.w_p = self.add_weight(name='w_p',\n",
        "                                       shape=(self.field_num * (self.field_num - 1) // 2, self.embed_dim,\n",
        "                                              self.embed_dim, hidden_units[0]),\n",
        "                                       initializer='random_uniform',\n",
        "                                       regularizer=l2(w_p_reg),\n",
        "                                       trainable=True)\n",
        "        self.l_b = self.add_weight(name='l_b', shape=(hidden_units[0], ),\n",
        "                                   initializer='random_uniform',\n",
        "                                   regularizer=l2(l_b_reg),\n",
        "                                   trainable=True)\n",
        "        # dnn\n",
        "        self.dnn_network = DNN(hidden_units[1:], activation, dnn_dropout)\n",
        "        self.dense_final = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        sparse_inputs = inputs\n",
        "        sparse_embed = [self.embed_layers['embed_{}'.format(i)](sparse_inputs[:, i])\n",
        "                 for i in range(sparse_inputs.shape[1])]\n",
        "        sparse_embed = tf.transpose(tf.convert_to_tensor(sparse_embed), [1, 0, 2])  # (None, field_num, embed_dim)\n",
        "        # product layer\n",
        "        row = []\n",
        "        col = []\n",
        "        for i in range(len(self.sparse_feature_columns) - 1):\n",
        "            for j in range(i + 1, len(self.sparse_feature_columns)):\n",
        "                row.append(i)\n",
        "                col.append(j)\n",
        "        p = tf.gather(sparse_embed, row, axis=1)\n",
        "        q = tf.gather(sparse_embed, col, axis=1)\n",
        "        if self.mode == 'in':\n",
        "            l_p = tf.tensordot(p*q, self.w_p, axes=2)  # (None, hidden[0])\n",
        "        else:  # out\n",
        "            u = tf.expand_dims(q, 2)  # (None, field_num(field_num-1)/2, 1, emb_dim)\n",
        "            v = tf.expand_dims(p, 2)  # (None, field_num(field_num-1)/2, 1, emb_dim)\n",
        "            l_p = tf.tensordot(tf.matmul(tf.transpose(u, [0, 1, 3, 2]), v), self.w_p, axes=3)  # (None, hidden[0])\n",
        "\n",
        "        l_z = tf.tensordot(sparse_embed, self.w_z, axes=2)  # (None, hidden[0])\n",
        "1➡️    l_1 = tf.nn.relu(tf.concat([l_z + l_p + self.l_b], axis=-1))\n",
        "        # dnn layer\n",
        "        dnn_x = self.dnn_network(l_1)\n",
        "        outputs = tf.nn.sigmoid(self.dense_final(dnn_x))\n",
        "        return outputs\n",
        "\n",
        "    def summary(self):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ Concatenating the sparse embeddings, product-layer dense embeddings, and bias, and the Relu activation on this feature vector.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/1547bd11bdf5feed5dc68cb2ff6d1a08) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Deep and Cross Network (DCN)\n",
        "\n",
        "In real world recommendation systems, we often have large and sparse feature space. So identifying effective feature processes in this setting would often require manual feature engineering or exhaustive search, which is highly inefficient. To tackle this issue, Google Research team has proposed Deep and Cross Network, DCN**.**\n",
        "\n",
        "It starts with an input layer, typically an embedding layer, followed by a cross network containing multiple cross layers that models explicitly feature interactions, and then combines with a deep network that models implicit feature interactions. The deep network is just a traditional multilayer construction. But the core of DCN is really the cross network. It explicitly applies feature crossing at each layer. And the highest polynomial degree increases with layer depth. The figure here shows the deep and cross layer in the mathematical form.\n",
        "\n",
        "There are a couple of ways to combine the cross network and the deep network:\n",
        "\n",
        "- Stack the deep network on top of the cross network.\n",
        "- Place deep & cross networks in parallel.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img9.png)\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class CrossNetwork(Layer):\n",
        "    def __init__(self, layer_num, reg_w=1e-6, reg_b=1e-6):\n",
        "        \"\"\"CrossNetwork\n",
        "        :param layer_num: A scalar. The depth of cross network\n",
        "        :param reg_w: A scalar. The regularizer of w\n",
        "        :param reg_b: A scalar. The regularizer of b\n",
        "        \"\"\"\n",
        "        super(CrossNetwork, self).__init__()\n",
        "        self.layer_num = layer_num\n",
        "        self.reg_w = reg_w\n",
        "        self.reg_b = reg_b\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = int(input_shape[-1])\n",
        "        self.cross_weights = [\n",
        "            self.add_weight(name='w_' + str(i),\n",
        "                            shape=(dim, 1),\n",
        "                            initializer='random_normal',\n",
        "                            regularizer=l2(self.reg_w),\n",
        "                            trainable=True\n",
        "                            )\n",
        "            for i in range(self.layer_num)]\n",
        "        self.cross_bias = [\n",
        "            self.add_weight(name='b_' + str(i),\n",
        "                            shape=(dim, 1),\n",
        "                            initializer='random_normal',\n",
        "                            regularizer=l2(self.reg_b),\n",
        "                            trainable=True\n",
        "                            )\n",
        "            for i in range(self.layer_num)]\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x_0 = tf.expand_dims(inputs, axis=2)  # (batch_size, dim, 1)\n",
        "        x_l = x_0  # (None, dim, 1)\n",
        "        for i in range(self.layer_num):\n",
        "            x_l1 = tf.tensordot(x_l, self.cross_weights[i], axes=[1, 0])  # (batch_size, dim, dim)\n",
        "            x_l = tf.matmul(x_0, x_l1) + self.cross_bias[i] + x_l  # (batch_size, dim, 1)\n",
        "        x_l = tf.squeeze(x_l, axis=2)  # (batch_size, dim)\n",
        "        return x_l\n",
        "\n",
        "class DNN(Layer):\n",
        "    def __init__(self, hidden_units, activation='relu', dropout=0.):\n",
        "        \"\"\"Deep Neural Network\n",
        "\t\t:param hidden_units: A list. Neural network hidden units.\n",
        "\t\t:param activation: A string. Activation function of dnn.\n",
        "\t\t:param dropout: A scalar. Dropout number.\n",
        "\t\t\"\"\"\n",
        "        super(DNN, self).__init__()\n",
        "        self.dnn_network = [Dense(units=unit, activation=activation) for unit in hidden_units]\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        for dnn in self.dnn_network:\n",
        "            x = dnn(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "```python\n",
        "class DCN(Model):\n",
        "    def __init__(self, feature_columns, hidden_units, activation='relu',\n",
        "                 dnn_dropout=0., embed_reg=1e-6, cross_w_reg=1e-6, cross_b_reg=1e-6):\n",
        "        \"\"\"\n",
        "        Deep&Cross Network\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param hidden_units: A list. Neural network hidden units.\n",
        "        :param activation: A string. Activation function of dnn.\n",
        "        :param dnn_dropout: A scalar. Dropout of dnn.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        :param cross_w_reg: A scalar. The regularizer of cross network.\n",
        "        :param cross_b_reg: A scalar. The regularizer of cross network.\n",
        "        \"\"\"\n",
        "        super(DCN, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.layer_num = len(hidden_units)\n",
        "        self.embed_layers = {\n",
        "            'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "                                         input_length=1,\n",
        "                                         output_dim=feat['embed_dim'],\n",
        "                                         embeddings_initializer='random_uniform',\n",
        "                                         embeddings_regularizer=l2(embed_reg))\n",
        "            for i, feat in enumerate(self.sparse_feature_columns)\n",
        "        }\n",
        "        self.cross_network = CrossNetwork(self.layer_num, cross_w_reg, cross_b_reg)\n",
        "        self.dnn_network = DNN(hidden_units, activation, dnn_dropout)\n",
        "        self.dense_final = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        sparse_inputs = inputs\n",
        "        sparse_embed = tf.concat([self.embed_layers['embed_{}'.format(i)](sparse_inputs[:, i])\n",
        "                                  for i in range(sparse_inputs.shape[1])], axis=-1)\n",
        "        x = sparse_embed\n",
        "        # Cross Network\n",
        "        cross_x = self.cross_network(x)\n",
        "        # DNN\n",
        "        dnn_x = self.dnn_network(x)\n",
        "        # Concatenate\n",
        "1➡️    total_x = tf.concat([cross_x, dnn_x], axis=-1)\n",
        "        outputs = tf.nn.sigmoid(self.dense_final(total_x))\n",
        "        return outputs\n",
        "\n",
        "    def summary(self):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ For each sparse feature, we first created an embedding vector and then passed the concatenated vector of these embeddings to the cross and deep network. Now, we are concatenating the outputs we received from these two networks and this vector we will pass to the final dense and sigmoid layer.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/c29b15a5b642f95b5c47dd24fcfbf534) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Neural Factorization Machine (NFM)\n",
        "\n",
        "NFM seamlessly combines the linearity of FM in modelling second-order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions. Conceptually, NFM is more expressive than FM since FM can be seen as a special case of NFM without hidden layers.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img10.png)\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class NFM(Model):\n",
        "    def __init__(self, feature_columns, hidden_units, dnn_dropout=0., activation='relu', bn_use=True, embed_reg=1e-6):\n",
        "        \"\"\"\n",
        "        NFM architecture\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param hidden_units: A list. Neural network hidden units.\n",
        "        :param activation: A string. Activation function of dnn.\n",
        "        :param dnn_dropout: A scalar. Dropout of dnn.\n",
        "        :param bn_use: A Boolean. Use BatchNormalization or not.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        \"\"\"\n",
        "        super(NFM, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.embed_layers = {\n",
        "            'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "                                         input_length=1,\n",
        "                                         output_dim=feat['embed_dim'],\n",
        "                                         embeddings_initializer='random_normal',\n",
        "                                         embeddings_regularizer=l2(embed_reg))\n",
        "            for i, feat in enumerate(self.sparse_feature_columns)\n",
        "        }\n",
        "        self.bn = BatchNormalization()\n",
        "        self.bn_use = bn_use\n",
        "        self.dnn_network = DNN(hidden_units, activation, dnn_dropout)\n",
        "        self.dense = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Inputs layer\n",
        "        sparse_inputs = inputs\n",
        "        # Embedding layer\n",
        "        sparse_embed = [self.embed_layers['embed_{}'.format(i)](sparse_inputs[:, i])\n",
        "                 for i in range(sparse_inputs.shape[1])]\n",
        "        sparse_embed = tf.transpose(tf.convert_to_tensor(sparse_embed), [1, 0, 2])  # (None, filed_num, embed_dim)\n",
        "        # Bi-Interaction Layer\n",
        "1➡️    sparse_embed = 0.5 * (tf.pow(tf.reduce_sum(sparse_embed, axis=1), 2) -\n",
        "                       tf.reduce_sum(tf.pow(sparse_embed, 2), axis=1))  # (None, embed_dim)\n",
        "        # Concat\n",
        "        x = sparse_embed\n",
        "        # BatchNormalization\n",
        "        x = self.bn(x, training=self.bn_use)\n",
        "        # Hidden Layers\n",
        "        x = self.dnn_network(x)\n",
        "        outputs = tf.nn.sigmoid(self.dense(x))\n",
        "        return outputs\n",
        "\n",
        "    def summary(self):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        tf.keras.Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ The bi-interaction pooling feature is being calculated as per this equation:\n",
        "\n",
        "$$f_{BI}(\\mathcal{V}_x) = \\dfrac{1}{2}\\left[ (\\sum_{i=1}^n x_iv_i)^2 - \\sum_{i=1}^n(x_iv_i)^2 \\right]$$\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/b16298b515077e07ad4dd4d2d575e430) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Attentional Factorization Machines (AFM)\n",
        "\n",
        "Improves FM by discriminating the importance of different feature interactions. It learns the importance of each feature interaction from data via a neural attention network. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters.\n",
        "\n",
        "![Untitled](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img11.png)\n",
        "\n",
        "Formally, the AFM model can be defined as:\n",
        "\n",
        "$$\\hat{y}_{AFM} (x) = w_0 + \\sum_{i=1}^nw_ix_i + p^T\\sum_{i=1}^n\\sum_{j=i+1}^na_{ij}(v_i\\odot v_j)x_ix_j$$\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class AFM(Model):\n",
        "    def __init__(self, feature_columns, mode, att_vector=8, activation='relu', dropout=0.5, embed_reg=1e-6):\n",
        "        \"\"\"\n",
        "        AFM \n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param mode: A string. 'max'(MAX Pooling) or 'avg'(Average Pooling) or 'att'(Attention)\n",
        "        :param att_vector: A scalar. attention vector.\n",
        "        :param activation: A string. Activation function of attention.\n",
        "        :param dropout: A scalar. Dropout.\n",
        "        :param embed_reg: A scalar. the regularizer of embedding\n",
        "        \"\"\"\n",
        "        super(AFM, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.mode = mode\n",
        "        self.embed_layers = {\n",
        "            'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "                                         input_length=1,\n",
        "                                         output_dim=feat['embed_dim'],\n",
        "                                         embeddings_initializer='random_uniform',\n",
        "                                         embeddings_regularizer=l2(embed_reg))\n",
        "            for i, feat in enumerate(self.sparse_feature_columns)\n",
        "        }\n",
        "        if self.mode == 'att':\n",
        "            self.attention_W = Dense(units=att_vector, activation=activation, use_bias=True)\n",
        "            self.attention_dense = Dense(units=1, activation=None)\n",
        "        self.dropout = Dropout(dropout)\n",
        "        self.dense = Dense(units=1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Input Layer\n",
        "        sparse_inputs = inputs\n",
        "        # Embedding Layer \n",
        "        embed = [self.embed_layers['embed_{}'.format(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
        "        embed = tf.transpose(tf.convert_to_tensor(embed), perm=[1, 0, 2])  # (None, len(sparse_inputs), embed_dim)\n",
        "        # Pair-wise Interaction Layer\n",
        "        row = []\n",
        "        col = []\n",
        "        for r, c in itertools.combinations(range(len(self.sparse_feature_columns)), 2):\n",
        "            row.append(r)\n",
        "            col.append(c)\n",
        "        p = tf.gather(embed, row, axis=1)  # (None, (len(sparse) * len(sparse) - 1) / 2, k)\n",
        "        q = tf.gather(embed, col, axis=1)  # (None, (len(sparse) * len(sparse) - 1) / 2, k)\n",
        "        bi_interaction = p * q  # (None, (len(sparse) * len(sparse) - 1) / 2, k)\n",
        "1➡️    # mode\n",
        "        if self.mode == 'max':\n",
        "            # MaxPooling Layer\n",
        "            x = tf.reduce_sum(bi_interaction, axis=1)   # (None, k)\n",
        "        elif self.mode == 'avg':\n",
        "            # AvgPooling Layer\n",
        "            x = tf.reduce_mean(bi_interaction, axis=1)  # (None, k)\n",
        "        else:\n",
        "            # Attention Layer\n",
        "            x = self.attention(bi_interaction)  # (None, k)\n",
        "        # Output Layer\n",
        "        outputs = tf.nn.sigmoid(self.dense(x))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def summary(self):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ The output feature vector from the bi-interaction layer can be pooled either using max or average mode or using attention mechanism.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/0a1aa59c812f5ed2b5fd6cd6760e81a9) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Deep Factorization Machines (DeepFM)\n",
        "\n",
        "DeepFM consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is a multi-layered perceptron that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction. It is worth pointing out that the spirit of DeepFM resembles that of the Wide & Deep architecture which can capture both memorization and generalization. The advantages of DeepFM over the Wide & Deep model is that it reduces the effort of hand-crafted feature engineering by identifying feature combinations automatically.\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img12.png](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img12.png)\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class DeepFM(Model):\n",
        "\tdef __init__(self, feature_columns, hidden_units=(200, 200, 200), dnn_dropout=0.,\n",
        "\t\t\t\t activation='relu', fm_w_reg=1e-6, embed_reg=1e-6):\n",
        "\t\t\"\"\"\n",
        "\t\tDeepFM\n",
        "\t\t:param feature_columns: A list. sparse column feature information.\n",
        "\t\t:param hidden_units: A list. A list of dnn hidden units.\n",
        "\t\t:param dnn_dropout: A scalar. Dropout of dnn.\n",
        "\t\t:param activation: A string. Activation function of dnn.\n",
        "\t\t:param fm_w_reg: A scalar. The regularizer of w in fm.\n",
        "\t\t:param embed_reg: A scalar. The regularizer of embedding.\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(DeepFM, self).__init__()\n",
        "\t\tself.sparse_feature_columns = feature_columns\n",
        "\t\tself.embed_layers = {\n",
        "\t\t\t'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "\t\t\t\t\t\t\t\t\t\t input_length=1,\n",
        "\t\t\t\t\t\t\t\t\t\t output_dim=feat['embed_dim'],\n",
        "\t\t\t\t\t\t\t\t\t\t embeddings_initializer='random_normal',\n",
        "\t\t\t\t\t\t\t\t\t\t embeddings_regularizer=l2(embed_reg))\n",
        "\t\t\tfor i, feat in enumerate(self.sparse_feature_columns)\n",
        "\t\t}\n",
        "\t\tself.index_mapping = []\n",
        "\t\tself.feature_length = 0\n",
        "\t\tfor feat in self.sparse_feature_columns:\n",
        "\t\t\tself.index_mapping.append(self.feature_length)\n",
        "\t\t\tself.feature_length += feat['feat_num']\n",
        "\t\tself.embed_dim = self.sparse_feature_columns[0]['embed_dim']  # all sparse features have the same embed_dim\n",
        "\t\tself.fm = FM(self.feature_length, fm_w_reg)\n",
        "\t\tself.dnn = DNN(hidden_units, activation, dnn_dropout)\n",
        "\t\tself.dense = Dense(1, activation=None)\n",
        "\n",
        "\tdef call(self, inputs, **kwargs):\n",
        "\t\tsparse_inputs = inputs\n",
        "\t\t# embedding\n",
        "\t\tsparse_embed = tf.concat([self.embed_layers['embed_{}'.format(i)](sparse_inputs[:, i])\n",
        "                                  for i in range(sparse_inputs.shape[1])], axis=-1)  # (batch_size, embed_dim * fields)\n",
        "\t\t# wide\n",
        "\t\tsparse_inputs = sparse_inputs + tf.convert_to_tensor(self.index_mapping)\n",
        "\t\twide_inputs = {'sparse_inputs': sparse_inputs,\n",
        "\t\t\t\t\t   'embed_inputs': tf.reshape(sparse_embed, shape=(-1, sparse_inputs.shape[1], self.embed_dim))}\n",
        "\t\twide_outputs = self.fm(wide_inputs)  # (batch_size, 1)\n",
        "\t\t# deep\n",
        "\t\tdeep_outputs = self.dnn(sparse_embed)\n",
        "\t\tdeep_outputs = self.dense(deep_outputs)  # (batch_size, 1)\n",
        "\t\t# outputs\n",
        "1➡️\toutputs = tf.nn.sigmoid(tf.add(wide_outputs, deep_outputs))\n",
        "\t\treturn outputs\n",
        "\n",
        "\tdef summary(self):\n",
        "\t\tsparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "\t\tModel(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ We are adding the outputs of FM layer and dense layer and passing through sigmoid activation to get the final model output.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/c4840698c73d1d7139147a654736fb2d) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. Extreme Deep Factorization Machines (xDeepFM)\n",
        "\n",
        "xDeepFM combines the CIN and a classical DNN into one unified model. xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly.\n",
        "\n",
        "![The architecture of xDeepFM.](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img13.png)\n",
        "\n",
        "The architecture of xDeepFM.\n",
        "\n",
        "Compressed Interaction Network (CIN) aims to generate feature interactions in an explicit fashion and at the vector-wise level. CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
        "\n",
        "![Components and architecture of the Compressed Interaction Network (CIN).](https://github.com/RecoHut-Stanzas/S021355/raw/main/images/img14.png)\n",
        "\n",
        "Components and architecture of the Compressed Interaction Network (CIN).\n",
        "\n",
        "Here is the implementation:\n",
        "\n",
        "```python\n",
        "class CIN(Layer):\n",
        "    def __init__(self, cin_size, l2_reg=1e-4):\n",
        "        \"\"\"CIN\n",
        "        :param cin_size: A list. [H_1, H_2 ,..., H_k], a list of the number of layers\n",
        "        :param l2_reg: A scalar. L2 regularization.\n",
        "        \"\"\"\n",
        "        super(CIN, self).__init__()\n",
        "        self.cin_size = cin_size\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # get the number of embedding fields\n",
        "        self.embedding_nums = input_shape[1]\n",
        "        # a list of the number of CIN\n",
        "        self.field_nums = [self.embedding_nums] + self.cin_size\n",
        "        # filters\n",
        "        self.cin_W = {\n",
        "            'CIN_W_' + str(i): self.add_weight(\n",
        "                name='CIN_W_' + str(i),\n",
        "                shape=(1, self.field_nums[0] * self.field_nums[i], self.field_nums[i + 1]),\n",
        "                initializer='random_normal',\n",
        "                regularizer=l2(self.l2_reg),\n",
        "                trainable=True)\n",
        "            for i in range(len(self.field_nums) - 1)\n",
        "        }\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        dim = inputs.shape[-1]\n",
        "        hidden_layers_results = [inputs]\n",
        "        # split dimension 2 for convenient calculation\n",
        "        split_X_0 = tf.split(hidden_layers_results[0], dim, 2)  # dim * (None, field_nums[0], 1)\n",
        "        for idx, size in enumerate(self.cin_size):\n",
        "            split_X_K = tf.split(hidden_layers_results[-1], dim, 2)  # dim * (None, filed_nums[i], 1)\n",
        "\n",
        "            result_1 = tf.matmul(split_X_0, split_X_K, transpose_b=True)  # (dim, None, field_nums[0], field_nums[i])\n",
        "\n",
        "            result_2 = tf.reshape(result_1, shape=[dim, -1, self.embedding_nums * self.field_nums[idx]])\n",
        "\n",
        "            result_3 = tf.transpose(result_2, perm=[1, 0, 2])  # (None, dim, field_nums[0] * field_nums[i])\n",
        "\n",
        "            result_4 = tf.nn.conv1d(input=result_3, filters=self.cin_W['CIN_W_' + str(idx)], stride=1,\n",
        "                                    padding='VALID')\n",
        "\n",
        "            result_5 = tf.transpose(result_4, perm=[0, 2, 1])  # (None, field_num[i+1], dim)\n",
        "\n",
        "            hidden_layers_results.append(result_5)\n",
        "\n",
        "        final_results = hidden_layers_results[1:]\n",
        "        result = tf.concat(final_results, axis=1)  # (None, H_1 + ... + H_K, dim)\n",
        "        result = tf.reduce_sum(result,  axis=-1)  # (None, dim)\n",
        "\n",
        "        return result\n",
        "```\n",
        "\n",
        "```python\n",
        "class xDeepFM(Model):\n",
        "    def __init__(self, feature_columns, hidden_units, cin_size, dnn_dropout=0, dnn_activation='relu',\n",
        "                 embed_reg=1e-6, cin_reg=1e-6, w_reg=1e-6):\n",
        "        \"\"\"\n",
        "        xDeepFM\n",
        "        :param feature_columns: A list. sparse column feature information.\n",
        "        :param hidden_units: A list. a list of dnn hidden units.\n",
        "        :param cin_size: A list. a list of the number of CIN layers.\n",
        "        :param dnn_dropout: A scalar. dropout of dnn.\n",
        "        :param dnn_activation: A string. activation function of dnn.\n",
        "        :param embed_reg: A scalar. The regularizer of embedding.\n",
        "        :param cin_reg: A scalar. The regularizer of cin.\n",
        "        :param w_reg: A scalar. The regularizer of Linear.\n",
        "        \"\"\"\n",
        "        super(xDeepFM, self).__init__()\n",
        "        self.sparse_feature_columns = feature_columns\n",
        "        self.embed_dim = self.sparse_feature_columns[0]['embed_dim']\n",
        "        self.embed_layers = {\n",
        "            'embed_' + str(i): Embedding(input_dim=feat['feat_num'],\n",
        "                                         input_length=1,\n",
        "                                         output_dim=feat['embed_dim'],\n",
        "                                         embeddings_initializer='random_normal',\n",
        "                                         embeddings_regularizer=l2(embed_reg))\n",
        "            for i, feat in enumerate(self.sparse_feature_columns)\n",
        "        }\n",
        "        self.index_mapping = []\n",
        "        self.feature_length = 0\n",
        "        for feat in self.sparse_feature_columns:\n",
        "            self.index_mapping.append(self.feature_length)\n",
        "            self.feature_length += feat['feat_num']\n",
        "        self.linear = Linear(self.feature_length, w_reg)\n",
        "        self.cin = CIN(cin_size=cin_size, l2_reg=cin_reg)\n",
        "        self.dnn = DNN(hidden_units=hidden_units, dnn_dropout=dnn_dropout, dnn_activation=dnn_activation)\n",
        "        self.cin_dense = Dense(1)\n",
        "        self.dnn_dense = Dense(1)\n",
        "        self.bias = self.add_weight(name='bias', shape=(1, ), initializer=tf.zeros_initializer())\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # Linear\n",
        "        linear_inputs = inputs + tf.convert_to_tensor(self.index_mapping)\n",
        "        linear_out = self.linear(linear_inputs)  # (batch_size, 1)\n",
        "        # cin\n",
        "        embed = [self.embed_layers['embed_{}'.format(i)](inputs[:, i]) for i in range(inputs.shape[1])]\n",
        "        embed_matrix = tf.transpose(tf.convert_to_tensor(embed), [1, 0, 2])\n",
        "        cin_out = self.cin(embed_matrix)  # (batch_size, dim)\n",
        "        cin_out = self.cin_dense(cin_out)  # (batch_size, 1)\n",
        "        # dnn\n",
        "        embed_vector = tf.reshape(embed_matrix, shape=(-1, embed_matrix.shape[1] * embed_matrix.shape[2]))\n",
        "        dnn_out = self.dnn(embed_vector)\n",
        "        dnn_out = self.dnn_dense(dnn_out)  # (batch_size, 1))\n",
        "        # output\n",
        "1➡️    output = tf.nn.sigmoid(linear_out + cin_out + dnn_out + self.bias)\n",
        "        return output\n",
        "\n",
        "    def summary(self):\n",
        "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
        "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()\n",
        "```\n",
        "\n",
        "1➡️ We are adding the output of CIN and DNN networks with the direct input features via Linear layer.\n",
        "\n",
        "[Here](https://nbviewer.org/gist/sparsh-ai/99b99eaecfac4b58ffa60603eb4bbd9c) is the Jupyter notebook of this experiment.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary/Footnotes\n",
        "\n",
        "1. There are 5 matching-class models and 10 ranking-class models.\n",
        "2. ML-1m dataset is used for training and evaluation of matching models, and Criteo dataset for ranking models.\n",
        "3. Jupyter notebook (with colab support) is available for each of these 15 experiments.\n",
        "4. Tensorflow version 2.5 is used for implementation of these models.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVM8CAnuvIV1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}